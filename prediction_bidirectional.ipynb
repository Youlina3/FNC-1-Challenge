{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate,Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model \n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=tf_config)\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder locations\n",
    "#W2V_DIR = './data/GoogleNews-vectors-negative300.bin' #\n",
    "GloVe_DIR = './data/glove.twitter.27B.50d.txt'\n",
    "#the data directory\n",
    "DATA_DIR = './data'\n",
    "# These are some hyperparameters that can be tuned\n",
    "MAX_SENT_LEN = 150 #75(0.68), 150, 300 700(90% but too time comsuming)\n",
    "MAX_VOCAB_SIZE = 1119000 #vocabulary\n",
    "LSTM_DIM = 100#len(embd[0])\n",
    "EMBEDDING_DIM = 50 #50 for GloVe 300 for w2v\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 40 #40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = pd.read_csv(DATA_DIR + '/body_table.csv')\n",
    "\n",
    "test_df = pd.read_csv(DATA_DIR + '/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.replace('unrelated',0,True)\n",
    "test_df.replace('agree',1,True)\n",
    "test_df.replace('disagree',2,True)\n",
    "test_df.replace('discuss',3,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df_test = test_df.join(bodies.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_head_test = [text_to_word_sequence(head) for head in combine_df_test['Headline']]\n",
    "word_seq_bodies_test = [text_to_word_sequence(body) for body in combine_df_test['articleBody']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = []\n",
    "\n",
    "for i in range(len(word_seq_head_test)):\n",
    "    word_seq.append(word_seq_head_test[i])\n",
    "for i in range(len(word_seq_bodies_test)):\n",
    "    word_seq.append(word_seq_bodies_test[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_test = [list(i) for i in word_seq_head_test]\n",
    "for i in range(len(word_seq_head_test)):\n",
    "    word_seq_test[i].extend(word_seq_bodies_test[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 10213\n"
     ]
    }
   ],
   "source": [
    "filter_list = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=filter_list)\n",
    "tokenizer.fit_on_texts([seq for seq in word_seq])\n",
    "\n",
    "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile Sentence of headline: 16.0\n"
     ]
    }
   ],
   "source": [
    "print('90th Percentile Sentence of headline:', np.percentile([len(seq) for seq in word_seq_head_test], 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile Sentence of body: 585.0\n"
     ]
    }
   ],
   "source": [
    "print('90th Percentile Sentence of body:', np.percentile([len(seq) for seq in word_seq_bodies_test], 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequence of words to sequnce of indices\n",
    "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_test])\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = './result/lstm/epoch40/Bidirectional_lstm_150token_lr0.001_trainable_39_0.9535.h5'\n",
    "model = models.load_model(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [np.argmax(p, axis = -1) for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outputs)):\n",
    "    if outputs[i] == 0: outputs[i] = \"unrelated\"\n",
    "    if outputs[i] == 1: outputs[i] = \"disagree\"\n",
    "    if outputs[i] == 2: outputs[i] = \"agree\"\n",
    "    if outputs[i] == 3: outputs[i] = \"discuss\"\n",
    "        \n",
    "df_predicted = pd.DataFrame({'Stance': outputs})\n",
    "result = pd.concat([test_df, df_predicted], axis=1, sort=False)\n",
    "result.to_csv('./bidrectional_answer.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
